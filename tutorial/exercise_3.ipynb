{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb2e5e8",
   "metadata": {},
   "source": [
    "# Exercise 3 - Templating\n",
    "This notebook shows an example of how to autogenerate C kernels that are specialized for a user requested platform. Doing so you can write Python scripts that take a user input and depending on that launch the C code either on CPU or GPU. This is useful as we have to write the kernel template only once without knowing what platform the future user might want to run our software on. As you will see, this is much simpler than it sounds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191fe95",
   "metadata": {},
   "source": [
    "In our example the kernel is a simple function that doubles the elements of a 1D vector. This is the same kernel from exercise 2, but with some extra comments added. These comments are markers where if requested, extra lines will be inserted specific to the parallel programming model CUDA or OpenCL, which will turn our serial C function into parallelized code for the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48051246",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sig = \"\"\"void elementwise(int, const double*, double*);\"\"\"\n",
    "source_str = r\"\"\"\n",
    "/*begin_gpukern*/\n",
    "void elementwise(int n, \n",
    "    /*gpuglmem*/ const double* x, \n",
    "    /*gpuglmem*/ double* y)\n",
    "{\n",
    "  for(int i=0; i<n; i++){//begin_parallel i n\n",
    "    y[i] = 2 * x[i];\n",
    "  }//end_parallel\n",
    "}\n",
    "/*end_gpukern*/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331de0bf",
   "metadata": {},
   "source": [
    "This is the user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0011c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "specialize_for = \"gpu_pyopencl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca4bbd",
   "metadata": {},
   "source": [
    "These fine tune the kernel for the requested platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lines = []  # here we collect the new kernel lines to be inserted\n",
    "\n",
    "for ll in source_str.splitlines():\n",
    "    if \"//begin_parallel\" in ll:\n",
    "        varname, limname = ll.split(\"//begin_parallel\")[-1].split()\n",
    "\n",
    "        if specialize_for == \"cpu\":\n",
    "                new_lines.append(\n",
    "                    f\"for (int {varname}=0; {varname}<{limname}; {varname}++)\"\n",
    "                    + \"{\\n\"\n",
    "                )\n",
    "\n",
    "        elif specialize_for == \"gpu_pyopencl\":\n",
    "            new_lines.append(f\"int {varname};\\n\")\n",
    "            new_lines.append(\n",
    "                f\"{varname}=get_global_id(0);\\n\"\n",
    "            )\n",
    "\n",
    "        elif specialize_for == \"gpu_cupy\":\n",
    "            new_lines.append(f\"int {varname};\\n\")\n",
    "            new_lines.append(\n",
    "                f\"{varname}=blockDim.x * blockIdx.x + threadIdx.x;\\n\"\n",
    "                f\"if ({varname}<{limname})\" + \"{\"\n",
    "            )\n",
    "            \n",
    "    elif \"//end_parallel\" in ll:\n",
    "        if specialize_for == \"cpu\":\n",
    "            new_lines.append(\"}\")\n",
    "        elif specialize_for == \"gpu_pyopencl\":\n",
    "            new_lines.append(\"\")\n",
    "        elif specialize_for == \"gpu_cupy\":\n",
    "            new_lines.append(\"}\")\n",
    "            \n",
    "    else:\n",
    "        new_lines.append(ll)\n",
    "        \n",
    "new_source_src = \"\\n\".join(new_lines)\n",
    "\n",
    "new_source_src = new_source_src.replace(\n",
    "    \"/*begin_gpukern*/\",\n",
    "    {\n",
    "        \"cpu\": \" \",\n",
    "        \"gpu_pyopencl\": \" __kernel \",\n",
    "        \"gpu_cupy\": \"extern \\\"C\\\"{\\n__global__\",\n",
    "    }[specialize_for],\n",
    ")\n",
    "\n",
    "new_source_src = new_source_src.replace(\n",
    "    \"/*end_gpukern*/\",\n",
    "    {\n",
    "        \"cpu\": \" \",\n",
    "        \"gpu_pyopencl\": \" \",\n",
    "        \"gpu_cupy\": \"}\",\n",
    "    }[specialize_for],\n",
    ")\n",
    "\n",
    "new_source_src = new_source_src.replace(\n",
    "    \"/*gpuglmem*/\",\n",
    "    {\n",
    "        \"cpu\": \" \",\n",
    "        \"gpu_pyopencl\": \" __global \",\n",
    "        \"gpu_cupy\": \" \",\n",
    "    }[specialize_for],\n",
    ")\n",
    "print(new_source_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c25ec",
   "metadata": {},
   "source": [
    "Build and load the kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 10\n",
    "x = np.random.randn(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if specialize_for == \"cpu\":\n",
    "    import cffi\n",
    "    print(\"using cffi\")\n",
    "    \n",
    "    ffi_interface = cffi.FFI()\n",
    "    ffi_interface.cdef(source_sig)\n",
    "    ffi_interface.set_source(\"_exercise_3\", new_source_src)\n",
    "    ffi_interface.compile(verbose=True)\n",
    "    \n",
    "    from _exercise_3 import ffi, lib\n",
    "    \n",
    "    x_cffi = ffi.cast( \"double *\", ffi.from_buffer(x))\n",
    "    y_cffi = ffi.new(\"double[]\", len(x))\n",
    "    lib.elementwise(10, x_cffi, y_cffi)\n",
    "    y = ffi.unpack(y_cffi, 10)\n",
    "    \n",
    "    assert np.allclose(x*2, y)\n",
    "    print(\"passed\")\n",
    "\n",
    "elif specialize_for == \"gpu_cupy\":\n",
    "    import cupy as cp\n",
    "    print(\"using cupy\")\n",
    "    \n",
    "    module = cp.RawModule(code=new_source_src)\n",
    "    elementwise_kernel = module.get_function(\"elementwise\")\n",
    "    \n",
    "    x_gpu = cp.array(x)\n",
    "    y_gpu = cp.zeros_like(x_gpu)\n",
    "    \n",
    "    blocksize = 1\n",
    "    n_blocks = int(np.ceil(len(x_gpu) / blocksize))\n",
    "    elementwise_kernel(grid=(n_blocks,), block=(blocksize,), args=(len(x), x_gpu, y_gpu))\n",
    "    y_cpu = y_gpu.get()\n",
    "    assert np.allclose(x*2, y_cpu)\n",
    "    print(\"passed\")\n",
    "    \n",
    "elif specialize_for == \"gpu_pyopencl\":\n",
    "    import pyopencl as cl\n",
    "    print(\"using pyopencl\")\n",
    "\n",
    "    ctx = cl.create_some_context(interactive=False)\n",
    "    queue = cl.CommandQueue(ctx)\n",
    "    \n",
    "    prg = cl.Program(ctx, new_source_src).build()\n",
    "    \n",
    "    x_gpu = cl.array.to_device(queue, x)\n",
    "    y_gpu = cl.array.zeros_like(x_gpu)\n",
    "\n",
    "    grid_size = len(x)\n",
    "    workgroup_size = 1\n",
    "    prg.elementwise(queue, (grid_size,), (workgroup_size,), np.int32(len(x)), x_gpu.data, y_gpu.data)\n",
    "    y_cpu = y_gpu.get()\n",
    "    assert np.allclose(x*2, y_cpu)\n",
    "    print(\"passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c5e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
