{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fab6c2",
   "metadata": {},
   "source": [
    "# Exercise 2b - PyOpenCL demo\n",
    "The goal of this demo is show the basic usecase of PyOpenCL. </br>\n",
    "We will perform a simple elementwise operation on the GPU in two ways: \n",
    "- using PyOpenCL's array API\n",
    "- manually by building and calling a C kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyopencl as cl\n",
    "import pyopencl.array as cl_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da22de",
   "metadata": {},
   "source": [
    "In PyopenCL we have to define our context manually. In the simplest case this means we select a platform, then a device, then initialize a context on this device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7706e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "platforms_list = cl.get_platforms()\n",
    "platforms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3360b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_list = platforms_list[0].get_devices()\n",
    "devices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5877555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the total amount of GPU memory in bytes\n",
    "devices_list[0].global_mem_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0999798",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = cl.Context(devices=devices_list)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab585b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = cl.CommandQueue(context)\n",
    "queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1eaf6c",
   "metadata": {},
   "source": [
    "Initialize an input array on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe545c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np.random.rand(int(1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f94fe",
   "metadata": {},
   "source": [
    "Transfer this array to the GPU. Specify the command queue in the first argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cl_array.to_device(queue, x_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9faa6",
   "metadata": {},
   "source": [
    "PyOpenCL arrays work like NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab1f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_cpu), type(x_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec50ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e58743",
   "metadata": {},
   "source": [
    "You can find the PyOpenCL equivalent of each NumPy math operation [here](https://documen.tician.de/pyopencl/array.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl.clmath as clmath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afbe546",
   "metadata": {},
   "source": [
    "An elementwise operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gpu = 2 * clmath.sin(x_gpu) + clmath.exp(x_gpu)\n",
    "print(type(y_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2899eca",
   "metadata": {},
   "source": [
    "A reduction operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6d4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_cpu = np.sum(x_cpu)\n",
    "z_gpu = cl_array.sum(x_gpu)\n",
    "print(z_cpu, type(z_cpu), z_gpu, type(z_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b79fbf",
   "metadata": {},
   "source": [
    "Transfer data back to the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d6529",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cpu = y_gpu.get()\n",
    "z_cpu = z_gpu.get()\n",
    "print(type(y_cpu), type(z_cpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c1e2c",
   "metadata": {},
   "source": [
    "We can do the same using manually defined low level C kernels\n",
    "\n",
    "__TODO:__ try to understand each line of the kernel, especially the indexing of the threads: `int i = get_global_id(0);`. How is it different compared to the CUDA kernel from Exercise 2a? Try to rewrite this line using OpenCL's local thread indexers: `get_group_id(0)`, `get_local_size(0)` and `get_local_id(0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_str = r\"\"\"\n",
    "__kernel\n",
    "void elementwise(\n",
    "    __global const double* x, \n",
    "    __global double* y)\n",
    "{\n",
    "    int i = get_global_id(0);\n",
    "    y[i] = 2 * sin(x[i]) + exp(x[i]);\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcffc9f",
   "metadata": {},
   "source": [
    "Build and load the kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prg = cl.Program(context, source_str).build()\n",
    "elementwise_kernel = prg.elementwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c9dc02",
   "metadata": {},
   "source": [
    "Define an output array on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gpu_2 = cl_array.zeros_like(x_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34037fb",
   "metadata": {},
   "source": [
    "To call the kernel function we define the global grid size and the size of the workgroup (equivalent to thread blocks in CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = len(x_cpu)\n",
    "workgroup_size = 1\n",
    "elementwise_kernel(queue, (grid_size,), (workgroup_size,), x_gpu.data, y_gpu_2.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fadd5e",
   "metadata": {},
   "source": [
    "Check that the two outputs (using the API and the C kernel) are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9975ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cpu_2 = y_gpu_2.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5be6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(y_cpu, y_cpu_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6123be2",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3833fa49",
   "metadata": {},
   "source": [
    "To profile the PyOpenCL code we have to enable it as a special property when creating the command queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31375b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = cl.CommandQueue(context, properties=cl.command_queue_properties.PROFILING_ENABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2b97f",
   "metadata": {},
   "source": [
    "Create the input and output buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c206150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prg = cl.Program(context, source_str).build()\n",
    "x_buffer = cl_array.to_device(queue, x_cpu)\n",
    "y_buffer = cl_array.zeros_like(x_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948ce5f",
   "metadata": {},
   "source": [
    "Set the grid and workgroup size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = len(x_cpu)\n",
    "workgroup_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02b484",
   "metadata": {},
   "source": [
    "To profile the kernel execution we mark the operation with an event (=a marker object in OpenCL, storing the state of the operation). Then we use `event.wait()` to synchronize the threads executing the command. Finally the timing information can be retrieved from the event object by `event.profile.start` and `event.profile.end`, which contain the GPU time in nanoseconds.\n",
    "\n",
    "__TODO:__ try to tune the grid and workgroup size to make the code run as fast as possible. (You might see different runtimes compared to the CuPy exercise due to the differences in the execution time of the math operations `sin` and `exp` by CUDA and OpenCL.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bfc65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = prg.elementwise(queue, (grid_size,), (workgroup_size,), x_buffer.data, y_buffer.data)\n",
    "event.wait()  #Â synchronize\n",
    "elapsed = (event.profile.end - event.profile.start)*1e-3 # convert from [ns] to [us]\n",
    "print(f\"GPU kernel time: {int(elapsed)} us\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4c586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
