{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb2e5e8",
   "metadata": {},
   "source": [
    "# Exercise 3 - Templating\n",
    "This notebook shows an example of how to autogenerate C kernels that are specialized for a user requested platform. Doing so you can write Python scripts that take a user input and depending on that launch the C code either on CPU or GPU. This is useful as we have to write the kernel template only once without knowing what platform the future user might want to run our software on. As you will see, this is much simpler than it sounds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191fe95",
   "metadata": {},
   "source": [
    "In our example in the cell below the kernel is a simple function that doubles the elements of a 1D vector. Notice that there are some extra comments added in the string: \n",
    "- `/*begin_gpukern*/`\n",
    "- `/*gpuglmem*/`\n",
    "- `//begin_parallel i n`\n",
    "- `//end_parallel`\n",
    "- `/*end_gpukern*/`\n",
    "\n",
    "\n",
    "These annotations are markers and we want to replace them by extra lines specific to the context. The context can be CPU or the parallel programming model CUDA or OpenCL for GPU. In the GPU context we want to automatically turn our serial C function into parallelized code and add the relevant kernel qualifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48051246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function signature\n",
    "source_sig = \"\"\"void elementwise(int, const double*, double*);\"\"\"\n",
    "\n",
    "# the function source\n",
    "source_str = r\"\"\"\n",
    "/*begin_gpukern*/\n",
    "void elementwise(int n, \n",
    "    /*gpuglmem*/ const double* x, \n",
    "    /*gpuglmem*/ double* y)\n",
    "{\n",
    "  for(int i=0; i<n; i++){//begin_parallel i n\n",
    "    y[i] = 2 * x[i];\n",
    "  }//end_parallel\n",
    "}\n",
    "/*end_gpukern*/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca4bbd",
   "metadata": {},
   "source": [
    "The `specialized_source()` function below fine tunes the above kernel for the requested context by replacing the annotations with the appropriate syntax (kernel qualifier, bracket, etc..).\n",
    "\n",
    "__TODO__: try to understand what the function does. Find the lines where each of the 5 annotations listed above are treated. Complete the missing parts of the function marked by __### your code here ###__ that replace the annotations in the source string above with the context specific syntax. You can use `new_lines.append(\"mystring\")` in the first part of the function and a simple string in the second part. The replacement of most annotations is already written, so you can take inspiration from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specialize_source(source_str, specialize_for):\n",
    "    new_lines = []  # here we collect the new kernel lines to be inserted\n",
    "    \n",
    "    # loop over the lines of the source string\n",
    "    for ll in source_str.splitlines():\n",
    "        \n",
    "        #Â fine tune the for loop\n",
    "        if \"//begin_parallel\" in ll:\n",
    "            varname, limname = ll.split(\"//begin_parallel\")[-1].split()\n",
    "    \n",
    "            if specialize_for == \"cpu\":\n",
    "                    new_lines.append(\n",
    "                        f\"for (int {varname}=0; {varname}<{limname}; {varname}++)\"\n",
    "                        + \"{\\n\"\n",
    "                    )\n",
    "                               \n",
    "            elif specialize_for == \"gpu_opencl\":\n",
    "                new_lines.append(f\"int {varname};\\n\")\n",
    "                new_lines.append(\n",
    "                    f\"{varname}=get_global_id(0);\\n\"\n",
    "                )\n",
    "                \n",
    "            elif specialize_for == \"gpu_cuda\":\n",
    "                new_lines.append(f\"int {varname};\\n\")\n",
    "                new_lines.append(\n",
    "                    f\"{varname}=blockDim.x * blockIdx.x + threadIdx.x;\\n\"\n",
    "                    f\"if ({varname}<{limname})\" + \"{\"\n",
    "                )\n",
    "                \n",
    "        elif \"//end_parallel\" in ll:\n",
    "            if specialize_for == \"cpu\":\n",
    "                ### your code here ###\n",
    "            elif specialize_for == \"gpu_opencl\":\n",
    "                ### your code here ###\n",
    "            elif specialize_for == \"gpu_cuda\":\n",
    "                ### your code here ###\n",
    "                \n",
    "        else:\n",
    "            new_lines.append(ll)\n",
    "            \n",
    "    new_source_src = \"\\n\".join(new_lines)\n",
    "    \n",
    "    # fine tune kernel qualifiers\n",
    "    new_source_src = new_source_src.replace(\n",
    "        \"/*begin_gpukern*/\",\n",
    "        {\n",
    "            \"cpu\": \" \",\n",
    "            \"gpu_opencl\": \" __kernel \",\n",
    "            \"gpu_cuda\": \"extern \\\"C\\\"{\\n__global__\",\n",
    "        }[specialize_for],\n",
    "    )\n",
    "    \n",
    "    new_source_src = new_source_src.replace(\n",
    "        \"/*end_gpukern*/\",\n",
    "        {\n",
    "            \"cpu\": ### your code here ###\n",
    "            \"gpu_opencl\": ### your code here ###\n",
    "            \"gpu_cuda\": ### your code here ###\n",
    "        }[specialize_for],\n",
    "    )\n",
    "    \n",
    "    new_source_src = new_source_src.replace(\n",
    "        \"/*gpuglmem*/\",\n",
    "        {\n",
    "            \"cpu\": ### your code here ###\n",
    "            \"gpu_opencl\": ### your code here ###\n",
    "            \"gpu_cuda\": ### your code here ###\n",
    "        }[specialize_for],\n",
    "    )\n",
    "    return new_source_src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c10d3d",
   "metadata": {},
   "source": [
    "Next, we create some input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 10\n",
    "x = np.random.randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58690a06",
   "metadata": {},
   "source": [
    "Now we specify the desired context: `cpu`, `gpu_cuda` or `gpu_opencl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d58b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "specialize_for = \"gpu_cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655d88f",
   "metadata": {},
   "source": [
    "Use our script to create the context specific kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89128f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_source_str = specialize_source(source_str, specialize_for)\n",
    "print(new_source_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd0fb2",
   "metadata": {},
   "source": [
    "Test the kernel. If everything is fine, the string \"passed\" should be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if specialize_for == \"cpu\":\n",
    "    import cffi\n",
    "    print(\"using cffi\")\n",
    "    \n",
    "    ffi_interface = cffi.FFI()\n",
    "    ffi_interface.cdef(source_sig)\n",
    "    ffi_interface.set_source(\"_exercise_3\", new_source_str)\n",
    "    ffi_interface.compile(verbose=True)\n",
    "    \n",
    "    from _exercise_3 import ffi, lib\n",
    "    \n",
    "    x_cffi = ffi.cast( \"double *\", ffi.from_buffer(x))\n",
    "    y_cffi = ffi.new(\"double[]\", len(x))\n",
    "    lib.elementwise(10, x_cffi, y_cffi)\n",
    "    y = ffi.unpack(y_cffi, 10)\n",
    "    \n",
    "    assert np.allclose(x*2, y)\n",
    "    print(\"passed\")\n",
    "\n",
    "elif specialize_for == \"gpu_cuda\":\n",
    "    import cupy as cp\n",
    "    print(\"using cupy\")\n",
    "    \n",
    "    module = cp.RawModule(code=new_source_str)\n",
    "    elementwise_kernel = module.get_function(\"elementwise\")\n",
    "    \n",
    "    x_gpu = cp.array(x)\n",
    "    y_gpu = cp.zeros_like(x_gpu)\n",
    "    \n",
    "    blocksize = 1\n",
    "    n_blocks = int(np.ceil(len(x_gpu) / blocksize))\n",
    "    elementwise_kernel(grid=(n_blocks,), block=(blocksize,), args=(len(x), x_gpu, y_gpu))\n",
    "    y_cpu = y_gpu.get()\n",
    "    assert np.allclose(x*2, y_cpu)\n",
    "    print(\"passed\")\n",
    "    \n",
    "elif specialize_for == \"gpu_opencl\":\n",
    "    import pyopencl as cl\n",
    "    from pyopencl import array as cl_array\n",
    "    print(\"using pyopencl\")\n",
    "\n",
    "    ctx = cl.create_some_context(interactive=False)\n",
    "    queue = cl.CommandQueue(ctx)\n",
    "    \n",
    "    prg = cl.Program(ctx, new_source_str).build()\n",
    "    \n",
    "    x_gpu = cl_array.to_device(queue, x)\n",
    "    y_gpu = cl_array.zeros_like(x_gpu)\n",
    "\n",
    "    grid_size = len(x)\n",
    "    workgroup_size = 1\n",
    "    prg.elementwise(queue, (grid_size,), (workgroup_size,), np.int32(len(x)), x_gpu.data, y_gpu.data)\n",
    "    y_cpu = y_gpu.get()\n",
    "    assert np.allclose(x*2, y_cpu)\n",
    "    print(\"passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c72661",
   "metadata": {},
   "source": [
    "Now imagine that we have a large software framework (e.g. a particle tracking framework) with the numerically heavy parts written in C, the rest written in Python. The framework will be used by many scientists on different computers, some with GPUs some with only CPUs. Using CFFI, CuPy and PyOpenCL we can design the framework to be \"multiplatform\" meaning that we write the simulation code only once, using template code in C and annotations similar to those in this exercise. In Python we can easily design wrapper classes or functions that specialize the C code for the specific platform, specified by the user depending on their needs and available computing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cf89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
